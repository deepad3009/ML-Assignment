{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "<h2>INFSCI 2595 Machine Learning - Fall 2018 </h2>\n",
    "<h1 style=\"font-size: 250%;\">Assignment #2</h1>\n",
    "<h3>Due Sunday, 11:59am, 11/18/2018</h3>\n",
    "<h3>Total points: 100 </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type in your information in the double quotes\n",
    "firstName = \"Deepa\"\n",
    "lastName = \"Dorairaj\"\n",
    "pittID = \"DED76\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <h3>  Problem #1. Linear Discriminant Analysis (LDA)and Quadratic Discriminant Analysis(QDA) </h3> \n",
    " ### [30 points]\n",
    " \n",
    "Do not write a code for this part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #1-1 [10 points]</h4>  <br>\n",
    "We have a classification problem with one feature $(x)$ and K classes to be classified. The prior probability of\n",
    "class $k$ is $ùúã_{k} = ùëÉ(ùëå = ùëò)$. Assume that the feature in class k has Gaussian distribution of\n",
    "mean $Œº_{k}$ and variance $œÉ^2 (ùí©(Œº,ùúé^{2}))$.The variance is the same for all classes. \n",
    "Prove that the Bayes‚Äô classifier (that chooses class k with largest $ùëÉ(ùëå = ùëò|ùë•))$ is equivalent to assigning an observation to the class for which the discriminant function $ùõø_{k}(x)$ is\n",
    "maximized, where \n",
    "\\begin{array} \\\\\n",
    "ùõø_{k}(x) = x\\frac{\\mu _{k}}{\\sigma ^{2}}- \\frac{\\mu_{2}^{k}}{2\\sigma ^{2}}+ log(\\pi _{k})\n",
    "\\end{array}\n",
    "<br> What is the name of this classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1-1\n",
    "#### The name of the above classifier is Linear Discriminant Analysis (LDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #1-2 [15 points]</h4>  <br>\n",
    "Extend **Problem #1-1** to include **p** features. With features from each class drawn from a\n",
    "Gaussian distribution with mean vector $Œº_{k}$ and covariance matrix $Œ£_{k}$ (which is now\n",
    "different for each class). What is the discriminant function that maximizes **ùëÉ(ùëå = ùëò|ùë•)**. Is\n",
    "the relationship with the feature vector **x** linear?<br> What is this classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1-2\n",
    "#### This classifier is Quadratic Discriminant Analysis (QDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #1-3 [5 points]</h4>  <br>\n",
    "- Explain Bias and variance trade-off between the two above classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The difference between the two classifiers is a bias-variance trade-off. With p predictors, estimating a co variance matrix requires estimating p(p+1)/2 parameters. \n",
    "#### ~ QDA estimates a separate co variance matrix for each class, so as the number of predictors becomes high, we experience a computational expense. Conversely, if we assume a common co variance matrix, we only have to do the computation once. \n",
    "#### ~ LDA is a much less flexible classifier, than QDA, thus has substantially lower variance. However, if the assumption of uniform variance is highly off, then LDA can suffer high bias. In general, LDA tends to be better than QDA if there are relatively few training observations, so therefore reducing variance is crucial. \n",
    "#### ~ QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern.\n",
    "#### Between Logistic regression LDA and QDA, the biggest things to take into consideration are the type of decision boundary that is required. If highly linear, than LDA and Logistic may prove superior, if non-linear, then QDA may be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <h3>  Problem #2. Regularization    </h3>\n",
    "### [15 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #2-1. [3 points]</h4> Answer the following questions \n",
    "\n",
    "- What is the main purpose of using regularization?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 2-1\n",
    "#### Regularization is mostly used in the field of classification and mainly prevents overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #2-2.Logistic Regression with Ridge Regularization [12 points] </h4>  <br>\n",
    "\n",
    "In this part, you should download and analyze the Wisconson **\"breast_cancer\"** dataset. <br>\n",
    "\n",
    "- Fit logistic regression model using ridge regularization with different values of  C = 0.1, 1, 5, 10, 50, 100, and 1000 (Note that C is the LogisticRegression function argument). For each value, report the estimated coefficients for the fitted model (do not just print summary, make a table with feature names and estimated coefficients)\n",
    "\n",
    "- What happens to the coefficients as you increase C?\n",
    "\n",
    "- What happens to the flexibility of the model as you increase C?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DataCancer=load_breast_cancer()\n",
    "print(DataCancer.keys())\n",
    "\n",
    "X_features=DataCancer.data\n",
    "Y_targetClass=DataCancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           0.1       1.0       5.0       10.0      50.0    \\\n",
      "mean radius              0.535436  1.722166  2.725981  3.058715  3.594745   \n",
      "mean texture             0.076798  0.089805  0.149395  0.125832  0.167928   \n",
      "mean perimeter           0.403791  0.105985 -0.119744 -0.101074 -0.203368   \n",
      "mean area               -0.012596 -0.007134 -0.005244 -0.011125 -0.011994   \n",
      "mean smoothness         -0.020002 -0.128409 -0.308702 -0.737346 -1.391057   \n",
      "mean compactness        -0.086440 -0.333501 -0.567312 -0.560991 -0.569680   \n",
      "mean concavity          -0.122754 -0.499364 -0.900420 -1.093522 -1.328377   \n",
      "mean concave points     -0.053632 -0.265081 -0.567216 -1.237483 -2.264764   \n",
      "mean symmetry           -0.039820 -0.267607 -0.651811 -1.558487 -2.846979   \n",
      "mean fractal dimension  -0.004791 -0.021495 -0.033193  0.047167  0.172525   \n",
      "radius error             0.022073  0.036473  0.013441  0.134060  0.362081   \n",
      "texture error            0.205684  0.986553  1.991618  1.760838  2.290579   \n",
      "perimeter error          0.035415  0.117088 -0.058260  0.145452 -0.145702   \n",
      "area error              -0.082756 -0.108717 -0.120894 -0.134175 -0.131936   \n",
      "smoothness error        -0.001268 -0.007966 -0.019853 -0.054764 -0.105355   \n",
      "compactness error       -0.013736  0.010561  0.139930  0.820745  1.882408   \n",
      "concavity error         -0.022317 -0.029188  0.071181  0.912204  2.308803   \n",
      "concave points error    -0.006676 -0.028183 -0.052716 -0.075594 -0.100429   \n",
      "symmetry error          -0.006716 -0.034314 -0.063310 -0.025484  0.079745   \n",
      "fractal dimension error -0.000807  0.008568  0.038786  0.169194  0.373305   \n",
      "worst radius             0.555264  1.358335  1.486690  1.137732  0.907997   \n",
      "worst texture           -0.204795 -0.289039 -0.409398 -0.387834 -0.455866   \n",
      "worst perimeter         -0.260319 -0.249833 -0.161803 -0.167726 -0.096863   \n",
      "worst area              -0.014497 -0.020124 -0.024568 -0.019998 -0.020625   \n",
      "worst smoothness        -0.036079 -0.216965 -0.515497 -1.205316 -2.269799   \n",
      "worst compactness       -0.273574 -1.027475 -1.783408 -1.601265 -1.552920   \n",
      "worst concavity         -0.358071 -1.447966 -2.650481 -2.910600 -3.187246   \n",
      "worst concave points    -0.108692 -0.533778 -1.144809 -2.421918 -4.378546   \n",
      "worst symmetry          -0.102373 -0.648564 -1.516599 -2.905937 -4.731520   \n",
      "worst fractal dimension -0.026370 -0.109135 -0.194787 -0.130892 -0.050116   \n",
      "\n",
      "                           100.0     1000.0  \n",
      "mean radius              3.810077  3.965055  \n",
      "mean texture             0.139510  0.131293  \n",
      "mean perimeter          -0.230790 -0.305086  \n",
      "mean area               -0.011577 -0.007306  \n",
      "mean smoothness         -1.291613 -1.580275  \n",
      "mean compactness        -0.636303 -0.630155  \n",
      "mean concavity          -1.271353 -1.432355  \n",
      "mean concave points     -2.117142 -2.568031  \n",
      "mean symmetry           -2.673083 -3.244819  \n",
      "mean fractal dimension   0.149036  0.200772  \n",
      "radius error             0.321066  0.404326  \n",
      "texture error            2.130158  2.072939  \n",
      "perimeter error          0.042259 -0.587077  \n",
      "area error              -0.140018 -0.097994  \n",
      "smoothness error        -0.097474 -0.120988  \n",
      "compactness error        1.697954  2.160878  \n",
      "concavity error          2.159846  2.706972  \n",
      "concave points error    -0.091623 -0.108310  \n",
      "symmetry error           0.042105  0.089324  \n",
      "fractal dimension error  0.336535  0.426860  \n",
      "worst radius             0.987308  0.891343  \n",
      "worst texture           -0.429037 -0.432130  \n",
      "worst perimeter         -0.119629 -0.030483  \n",
      "worst area              -0.020483 -0.027104  \n",
      "worst smoothness        -2.108683 -2.573837  \n",
      "worst compactness       -1.699781 -1.661258  \n",
      "worst concavity         -2.878247 -3.219149  \n",
      "worst concave points    -4.067834 -4.931804  \n",
      "worst symmetry          -4.571307 -5.359487  \n",
      "worst fractal dimension -0.075698 -0.042809  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test= train_test_split(X_features, Y_targetClass, random_state= 0)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "table = pd.DataFrame(columns = [0.1, 1, 5, 10, 50, 100, 1000], index = DataCancer.feature_names)\n",
    "for c in [0.1, 1, 5, 10, 50, 100, 1000]:\n",
    "    LogRegModel1 = LogisticRegression(C=c).fit(X_train, Y_train)\n",
    "    coef = LogRegModel1.coef_\n",
    "    table[c]=coef.T\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 2-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚Ä¢ Large C means less regularization strength\n",
    "#### ‚Ä¢ very large C means no regularization\n",
    "#### ‚Ä¢ Small C means more regularization and coefficients will be close to zero\n",
    "#### ‚Ä¢ C is inversely proportional to alpha in the regression functions, hence smaller C specifies stronger regularization. Therefore, Coefficients decrease when C increases.\n",
    "#### ‚Ä¢ The model is flexible when C increases, Flexibility of the model reduces when C is very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h3>  Problem #3. Logistic Regression and Unbalanced Datasets  </h3> \n",
    "### [25 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We fit a logistic regression model to predict the probability that an individual will default on his/her credit card balance. We used the total balance (single feature) to fit the model and got the results shown in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-1. [5 points]</h4> Prediciton with Logistic regression <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Table|Coefficient|Std.error|Z-statistic|P-Value|\n",
    "|:--:|:-------------------------------:|\n",
    "|Intercept|-10.6513|0.3612|-29.5|<0.0001|\n",
    "|balance|0.0055|0.002|24.9|<0.0001|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(You can refer to page 14 in the our slides \"Logistic Regression.pdf\" if the above table does not show well.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the parametric model used in logistic regression?\n",
    "- What is the class label of an individual with a balance equals to 15,000 dollar? What is the class label of an individual with balance equals to 800 dollar? (Write a python function which takes two inputs (feature, model_parameters) and returns the class labels for the data). default is class 1 and non-default is class 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3-1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚Ä¢ The parameter model used in logistic regression has a logit that is linear with X(feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for balance 15000 dollar, label is\n",
      "1, high probability to default\n",
      "and\n",
      "for balance 800 dollar, label is\n",
      "0, low probability to default\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def label(bal,coef):\n",
    "    intercept = coef[0]\n",
    "    coef = coef[1]\n",
    "    p = math.exp(intercept + coef * bal)/(1+math.exp(intercept + coef * bal))\n",
    "    print('for balance %d dollar, label is' %bal)\n",
    "    if p>= 0.5:\n",
    "        print('1, high probability to default')\n",
    "    else:\n",
    "        print('0, low probability to default')\n",
    "coef = [-10.6513,0.0055]\n",
    "label(15000, coef)\n",
    "print('and')\n",
    "label(800, coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-2. [10 points]</h4>  <br>\n",
    "\n",
    "The coefficients of logistic regression are obtained by maximizing the likelihood function\n",
    "\n",
    "\\begin{array} \\\\\n",
    "l(\\beta) = \\prod_{i:y_{i}=1} P(y_{i} = 1|x)\\prod_{i{}':y_{{i}'}=0} (1-P(y_{{i}'} = 1|x))\n",
    "\\end{array}\n",
    "Show that maximizing the\n",
    "likelihood function is equivalent to minimizing the cost function $J(\\beta)$, such that.\n",
    "\\begin{array} \\\\\n",
    "J(\\beta) = -\\sum [y_{i} log(P(y_{i} = 1|x)) + (1- y_{i})log(1- P(y_{i} = 1|x))]\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "Here $n$ is the number training examples. Mention one possible method for obtaining the\n",
    "optimal coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #3-3 [10 points]</h4> <br>\n",
    "In a fraud detection system, a QDA classifier‚Äôs confusion matrix is found to be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|        |Predicted Class - Not fraud| Predicted Class - fraud|\n",
    "|:--:|:-------------------------------:|\n",
    "|Actual class ‚Äì Not fraud|1200|25|\n",
    "|Actual class ‚Äì fraud|30|7|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Is dataset balanced? Why?\n",
    "- Evaluate the overall error rate <br>\n",
    "- Evaluate the precision and the recall <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3-3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The given dataset is not balanced because majority of cases are not fraud and ver small minority of cases are under fraud. A normal balanced dataset will have 50/50 or 60/40 distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the above confusion matrix\n",
    "TN = 1200\n",
    "FP = 25\n",
    "FN = 30\n",
    "TP = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043581616481774964"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Error rate\n",
    "(FN+FP)/(TP+TN+FN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1891891891891892"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RECALL\n",
    "TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21875"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PRECISION\n",
    "TP/(TP+FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <h3>  Problem #4. SVM, Decision Trees, MLP Classification   </h3> \n",
    "### [30 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you will use different classification methods, SVM, Decision Tree and MLP; and find their accuracies using the test data. \n",
    "We will also use the Wisconson **\"breast_cancer\"** dataset.\n",
    "In all of the following subparts, use random_state=0 when you split the dataset into train and test and **standardize** the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets \n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.datasets import load_breast_cancer\n",
    "  \n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(dataset.data, dataset.target, random_state= 0)\n",
    "\n",
    "# standardize data\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_transformed= scaler.transform(X_train)\n",
    "X_test_transformed= scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #4-1.  Classification with SVM [10 points]</h4><br>\n",
    "- How does the Radial Basis Function Kernel in SVM measure the similarity between a test point and a training example?\n",
    "- Fit an SVM classifier with a radial basis function kernel, with gamma =0.1, and regularization parameter C set to 10. Use the classifier to predict class labels for the test data. \n",
    "- Calculate the accuracy and confusion matrix on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the RBF kernel decreases with distance and ranges between zero and one, support vector machines and other models employing the kernel trick do not scale well to large numbers of training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svmModel=SVC(kernel='rbf', gamma=0.1, C=10).fit(X_train_transformed, y_train)\n",
    "svmModel.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[52,  1],\n",
       "       [ 4, 86]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = svmModel.predict(X_test_transformed)\n",
    "# confusion matrix on test data\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.965034965034965"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "# accuracy on test data\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class labels for test data\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #4-2.  Classification with Decisin Tree (DT) [10 points]</h4><br>\n",
    "- In this part, use DT classification method on the training data. Set the maximum depth of the tree to five. Then use the classifier to predict class labels for the test data. Calculate the accuracy and confusion matrix on the test data.\n",
    "- Use Adaboost to combine four decision trees each of max_depth of five. Use random_state=0 in adaboost. Find the test accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9300699300699301"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "treeModel=DecisionTreeClassifier(max_depth=5)\n",
    "treeModel.fit(X_train_transformed,y_train)\n",
    "treeModel.score(X_test_transformed,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[51,  2],\n",
       "       [ 8, 82]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred1 = treeModel.predict(X_test_transformed)\n",
    "# confusion matrix on test data\n",
    "confusion_matrix(y_test, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9300699300699301"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test accuracy\n",
    "accuracy_score(y_test,y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9370629370629371"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "BoostModel= AdaBoostClassifier(n_estimators=4, base_estimator = DecisionTreeClassifier(max_depth=5),random_state=0)\n",
    "BoostModel.fit(X_train_transformed,y_train)\n",
    "y_pred2 = BoostModel.predict(X_test_transformed)\n",
    "# test accuracy\n",
    "accuracy_score(y_test,y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Problem #4-3 [10 points]</h4>\n",
    "\n",
    "Follow steps to answer questions to build a neural network using MLPClassifier from sklearn.neural_network. \n",
    "- Build a model that has two hidden layers, the first layer has 10 neurons and second layer has 5 neurons. \n",
    "- Use 'relu' activation function, and set the regularization parameter alpha=0.5. \n",
    "- Set max_iter=1000; Set the random_state=0.\n",
    "- Use stochastic gradient descent (sgd) to solve the optimization  problem\n",
    "- Report accuracy, confusion matrix, precision, and recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "MLPmodelc=MLPClassifier(solver='sgd', max_iter = 1000, activation='relu', random_state=0, \n",
    "                       hidden_layer_sizes=[10,5], alpha=0.5).fit(X_train_transformed,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[52,  1],\n",
       "       [ 2, 88]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred11 = MLPmodelc.predict(X_test_transformed)\n",
    "# confusion matrix\n",
    "confusion_matrix(y_test, y_pred11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9790209790209791"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "accuracy_score(y_test,y_pred11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.98      0.97        53\n",
      "          1       0.99      0.98      0.98        90\n",
      "\n",
      "avg / total       0.98      0.98      0.98       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred11))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
